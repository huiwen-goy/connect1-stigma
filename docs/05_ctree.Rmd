---
title: "Classification tree (CART)"

output: 
  html_document:
    toc: TRUE
    toc_depth: 2
---

<a id="top"></a>

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

# About classification trees  
A classification tree uses predictors to sequentially sort observations into more homogeneous categories of outcomes, with the goal of knowing what sequence of labels or cut-off values ultimately leads to an outcome of "Yes" or "No". This different from logistic regression, which uses a weighted combination of predictors to calculate the probability of an outcome being "Yes" or "No".  
  
**Advantages**:  
A classification tree has a built-in variable selection procedure, and it can handle different types of predictors easily (categorical and numerical). Compared to logistic regression, the information from a classification tree is easier to interpret. A toy example: 20 out of a sample of 100 people bought hearing aids and 80 did not, and 19 of those 20 were correctly identified using the cut-off values of PTA > 40 followed by HHIE > 20. This information is more practically useful than knowing, say, that every 1-point increase in PTA led to a 3% increase in the odds of someone buying a hearing aid. Unlike logistic regression, trees can work around missing data by using "surrogate variables" in place of missing variables, whereas logistic regression requires all variables to be complete.  
  
**Disadvantages**:  
A single classification tree is not as "robust" as some other models. That is, a small change in the data can lead to a big change in the model, because earlier branches in the sequence will have flow-on effects. Simplicity can also be its downfall. First, using labels and cut-offs to classify cases isn't very flexible for fitting complex data and can make trees less accurate than some other types of models. Second, allowing fewer branches to grow makes a tree easier to interpret, but it is likely to classify fewer cases correctly.  

```{r, include=FALSE, message=FALSE}

# Libraries
library(caret) #for confusionMatrix
library(pROC) #AUC
library(rpart)
library(rpart.plot)
library(rattle)
library(ggplot2)
library(knitr)

# Read raw data
orig <- read.csv("/Users/huiwen/Documents/Research/Sonova_1_Stigma_and_Connect_Hearing/dataset/Connect_Data_20191025.csv", header=TRUE)

# Must have known age and sex, 50+ years old, never used a HA
data <- orig[which(is.na(orig$Q44) == FALSE & is.na(orig$Q43) == FALSE & orig$Q44 > 49.5 & orig$Q42 == "No"), ]

# Recalculate Age_stigma_avg without Q4, replacing the existing variable
data$Age_stigma_avg <- rowMeans(data[, c("Q5_Corrected_reversed", "Q6_Corrected_reversed", "Q7_Corrected_reversed", "Q8_Corrected_reversed")])

# Rename variables, recode to numeric
data$Age <- data$Q44

data$Sex <- c(NA)
data$Sex[data$Q43 == "Male"] <- 1
data$Sex[data$Q43 == "Female"] <- 0

data$Edu <- data$Q46_Corrected
data$Health <- data$Q53
data$QoL <- data$Q54

data$Married <- c(NA) 
data$Married[data$marital_recoded == "Married"] <- 1
data$Married[data$marital_recoded == "Sep_Divor"] <- 0
data$Married[data$marital_recoded == "Single"] <- 0
data$Married[data$marital_recoded == "Widowed"] <- 0
data$Married[data$marital_recoded == "Other"] <- 0

# Hearing ability scale; PTABE already calculated
data$Ability <- data$Q56_Corrected

data$Accomp <- c(NA)
data$Accomp[data$Q45 == "Yes"] <- 1
data$Accomp[data$Q45 == "No"] <- 0

data$Help_neighbours <- data$Q50_Corrected

data$Help_problems <- c(NA)
data$Help_problems[is.na(data$Q51)==FALSE & data$Q51 == "0"] <- 0
data$Help_problems[is.na(data$Q51)==FALSE & data$Q51 == "1_2"] <- 1
data$Help_problems[is.na(data$Q51)==FALSE & data$Q51 == "3_5"] <- 2
data$Help_problems[is.na(data$Q51)==FALSE & data$Q51 == "5+"] <- 3

data$Concern <- data$Q52
data$Lonely <- data$Q55

# Q21
data$Soc_Suspect_HL <- c(NA)
data$Soc_Suspect_HL[data$Q21_Corrected != "0" & is.na(data$Q21_Corrected)==FALSE] <- 1
data$Soc_Suspect_HL[data$Q21_Corrected == "0" & is.na(data$Q21_Corrected)==FALSE] <- 0 
# Q22
data$Soc_Know_HL <- c(NA)
data$Soc_Know_HL[data$Q22_Corrected != "0" & is.na(data$Q22_Corrected)==FALSE] <- 1
data$Soc_Know_HL[data$Q22_Corrected == "0" & is.na(data$Q22_Corrected)==FALSE] <- 0 

# Q23
data$Soc_Discuss_HL <- c(NA)
data$Soc_Discuss_HL[data$Q23_Corrected != "0" & is.na(data$Q23_Corrected)==FALSE] <- 1
data$Soc_Discuss_HL[data$Q23_Corrected == "0" & is.na(data$Q23_Corrected)==FALSE] <- 0 
# Q24
data$Soc_Hearing_test <- c(NA)
data$Soc_Hearing_test[data$Q24_Corrected != "0" & is.na(data$Q24_Corrected)==FALSE] <- 1
data$Soc_Hearing_test[data$Q24_Corrected == "0" & is.na(data$Q24_Corrected)==FALSE] <- 0 

# Q25
data$Soc_Obtain_HA <- c(NA)
data$Soc_Obtain_HA[data$Q25_Corrected != "0" & is.na(data$Q25_Corrected)==FALSE] <- 1
data$Soc_Obtain_HA[data$Q25_Corrected == "0" & is.na(data$Q25_Corrected)==FALSE] <- 0 

# Q26
data$Soc_Sometimes_use <- c(NA)
data$Soc_Sometimes_use[data$Q26_Corrected != "0" & is.na(data$Q26_Corrected)==FALSE] <- 1
data$Soc_Sometimes_use[data$Q26_Corrected == "0" & is.na(data$Q26_Corrected)==FALSE] <- 0 

# Q27
data$Soc_Regular_use <- c(NA)
data$Soc_Regular_use[data$Q27_Corrected != "0" & is.na(data$Q27_Corrected)==FALSE] <- 1
data$Soc_Regular_use[data$Q27_Corrected == "0" & is.na(data$Q27_Corrected)==FALSE] <- 0 

# Q28
data$Soc_Very_positive <- c(NA)
data$Soc_Very_positive[data$Q28_Corrected != "0" & is.na(data$Q28_Corrected)==FALSE] <- 1
data$Soc_Very_positive[data$Q28_Corrected == "0" & is.na(data$Q28_Corrected)==FALSE] <- 0

# Q29
data$Soc_Somewhat_positive <- c(NA)
data$Soc_Somewhat_positive[data$Q29_Corrected != "0" & is.na(data$Q29_Corrected)==FALSE] <- 1
data$Soc_Somewhat_positive[data$Q29_Corrected == "0" & is.na(data$Q29_Corrected)==FALSE] <- 0 
# Q30
data$Soc_Somewhat_negative <- c(NA)
data$Soc_Somewhat_negative[data$Q30_Corrected != "0" & is.na(data$Q30_Corrected)==FALSE] <- 1
data$Soc_Somewhat_negative[data$Q30_Corrected == "0" & is.na(data$Q30_Corrected)==FALSE] <- 0 
# Q31
data$Soc_Very_negative <- c(NA)
data$Soc_Very_negative[data$Q31_Corrected != "0" & is.na(data$Q31_Corrected)==FALSE] <- 1
data$Soc_Very_negative[data$Q31_Corrected == "0" & is.na(data$Q31_Corrected)==FALSE] <- 0 

# Make outcome measure into 0/1
data$Purchased_HA <- c(NA)
data$Purchased_HA[data$HA.Purchase == "Yes" & is.na(data$HA.Purchase)==FALSE] <- 1
data$Purchased_HA[data$HA.Purchase == "No" & is.na(data$HA.Purchase)==FALSE] <- 0

# make into factors
data$Sex.f <- factor(data$Q43, levels=c("Female", "Male"))
data$Accomp.f <- factor(data$Q45, levels=c("No", "Yes"))
data$Married.f <- as.factor(data$Married)
  levels(data$Married.f) <- c("No", "Yes")
data$Soc_Suspect_HL.f <- as.factor(data$Soc_Suspect_HL)
	levels(data$Soc_Suspect_HL.f) <- c("No", "Yes")
data$Soc_Know_HL.f <- as.factor(data$Soc_Know_HL)
	levels(data$Soc_Know_HL.f) <- c("No", "Yes")
data$Soc_Discuss_HL.f <- as.factor(data$Soc_Discuss_HL)
	levels(data$Soc_Discuss_HL.f) <- c("No", "Yes")
data$Soc_Hearing_test.f <- as.factor(data$Soc_Hearing_test)
	levels(data$Soc_Hearing_test.f) <- c("No", "Yes")
data$Soc_Obtain_HA.f <- as.factor(data$Soc_Obtain_HA)
	levels(data$Soc_Obtain_HA.f) <- c("No", "Yes")
data$Soc_Sometimes_use.f <- as.factor(data$Soc_Sometimes_use)
	levels(data$Soc_Sometimes_use.f) <- c("No", "Yes")
data$Soc_Regular_use.f <- as.factor(data$Soc_Regular_use)
	levels(data$Soc_Regular_use.f) <- c("No", "Yes")
data$Soc_Very_positive.f <- as.factor(data$Soc_Very_positive)
	levels(data$Soc_Very_positive.f) <- c("No", "Yes")
data$Soc_Somewhat_positive.f <- as.factor(data$Soc_Somewhat_positive)
	levels(data$Soc_Somewhat_positive.f) <- c("No", "Yes")
data$Soc_Somewhat_negative.f <- as.factor(data$Soc_Somewhat_negative)
	levels(data$Soc_Somewhat_negative.f) <- c("No", "Yes")
data$Soc_Very_negative.f <- as.factor(data$Soc_Very_negative)
	levels(data$Soc_Very_negative.f) <- c("No", "Yes")
data$HA.Purchase <- factor(data$HA.Purchase, levels=c("No", "Yes"))

# New: relabel individual items in scales
data$SubAge_1 <- data$Q1_Corrected
data$SubAge_2 <- data$Q2_Corrected 
data$SubAge_3 <- data$Q3_Corrected

data$AgeStigma_1 <- data$Q4_Corrected 
data$AgeStigma_2 <- data$Q5_Corrected 
data$AgeStigma_3 <- data$Q6_Corrected 
data$AgeStigma_4 <- data$Q7_Corrected
data$AgeStigma_5 <- data$Q8_Corrected

data$HaStigma_1 <- data$Q9_Corrected 
data$HaStigma_2 <- data$Q10_Corrected 
data$HaStigma_3 <- data$Q11_Corrected 
data$HaStigma_4 <- data$Q12_Corrected

# PTA-BE > 25 and complete data
pta <- subset(data, 
  PTA4_better_ear > 25 &           
  is.na(data$Age)==F & 
  is.na(data$PTA4_better_ear)==F & 
  is.na(data$HHIE_total)==F & 
  is.na(data$Ability)==F &  
  is.na(data$Sex)==F & 
  is.na(data$Edu)==F &  
  is.na(data$Married)==F &  
  is.na(data$Health)==F & 
  is.na(data$QoL)==F & 
  is.na(data$Help_neighbours)==F &  
  is.na(data$Help_problems)==F &
  is.na(data$Concern)==F & 
  is.na(data$Lonely)==F &   
  is.na(data$SubAge_1)==F &  
    is.na(data$SubAge_2)==F & 
    is.na(data$SubAge_3)==F & 
  is.na(data$AgeStigma_1)==F &
    is.na(data$AgeStigma_2)==F &
    is.na(data$AgeStigma_3)==F &
    is.na(data$AgeStigma_4)==F &
    is.na(data$AgeStigma_5)==F &
  is.na(data$HaStigma_1)==F & 
    is.na(data$HaStigma_2)==F &
    is.na(data$HaStigma_3)==F &
    is.na(data$HaStigma_4)==F &
  is.na(data$Accomp)==F & 
  is.na(data$Soc_Suspect_HL)==F &
  is.na(data$Soc_Know_HL)==F &
  is.na(data$Soc_Discuss_HL)==F &
  is.na(data$Soc_Hearing_test)==F &
  is.na(data$Soc_Obtain_HA)==F &   
  is.na(data$Soc_Sometimes_use)==F &
  is.na(data$Soc_Regular_use)==F &
  is.na(data$Soc_Very_positive)==F &
  is.na(data$Soc_Somewhat_positive)==F &
  is.na(data$Soc_Somewhat_negative)==F &
  is.na(data$Soc_Very_negative)==F)

rm(orig)
rm(data)


```

# Using case weights  

[back to top](#top)  

The ratio of "Yes" to "No" cases was 149:604. "No" cases were assigned lighter weights (149/604 = 0.2466887) than "Yes" cases (1.0), so that the model would not be biased towards "No" cases. It's as if there were equal numbers of each class (149 each) at the top of the tree.  

```{r, echo=FALSE, message=FALSE}

# Add case weights
pta$caseweights <- ifelse(pta$HA.Purchase == "No", 0.2466887, 1)

# Standard formula, with predictors as factors, and outcome as string
formula28x <- formula(HA.Purchase ~ 
  Age +  
  PTA4_better_ear +
  HHIE_total +
  Ability +
  Sex.f +
  Edu +
  Married.f +
  Health +
  QoL +
  Help_neighbours +
  Help_problems +
  Concern +
  Lonely +
  Sub_Age_avg +
  Age_stigma_avg +
  HA_stigma_avg +
  Accomp.f +
  Soc_Suspect_HL.f +
  Soc_Know_HL.f +
  Soc_Discuss_HL.f +
  Soc_Hearing_test.f +
  Soc_Obtain_HA.f +
  Soc_Sometimes_use.f +
  Soc_Regular_use.f +
  Soc_Very_positive.f +
  Soc_Somewhat_positive.f +
  Soc_Somewhat_negative.f +
  Soc_Very_negative.f)

```

*Technical note: A loss matrix can be specified for the tree model, which adds a penalty for either false positives or missed true cases. Using a loss matrix instead of, or in addition to, case weights doesn't make a difference for sensitivity or specificity. Only case weights seem to matter in this analysis.*   

```{r, include=FALSE, evaluate=FALSE}

# Effects of case weights and applying different types of loss matrix

# full tree, with case weights
m_cp0 <- rpart(formula28x, data = pta, 
               weights = pta$caseweights, 
               method = "class", 
               parms = list(split = "gini"), 
               control = rpart.control(cp = 0))

m_cp0_pred <- predict(m_cp0, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp0_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))

#         Reference
#Prediction  No Yes
#       No  460  15
#       Yes 144 134
#               Accuracy : 0.7888          
#                 95% CI : (0.7579, 0.8175)
#            Sensitivity : 0.8993          
#            Specificity : 0.7616

printcp(m_cp0)
plotcp(m_cp0, upper="splits")


# cp = 0.014 with case weights; fairly simple, 5-split tree
m_cp014 <- rpart(formula28x, data = pta, 
               weights = pta$caseweights, 
               method = "class", 
               parms = list(split = "gini"), 
               control = rpart.control(cp = 0.014))

m_cp014_pred <- predict(m_cp014, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))

#         Reference
#Prediction  No Yes
#       No  441  61
#       Yes 163  88
#   Accuracy : 0.7025 
#Sensitivity : 0.5906         
#Specificity : 0.7301
            
printcp(m_cp014)
plotcp(m_cp014, upper="splits")
rpart.plot(m_cp014)


# cp = 0.014, NO case weights
m_cp014_now <- rpart(formula28x, data = pta, 
               method = "class", 
               parms = list(split = "gini"), 
               control = rpart.control(cp = 0.014))

m_cp014_now_pred <- predict(m_cp014_now, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_now_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))
#          Reference
#Prediction  No Yes
#       No  604 149
#       Yes   0   0
#   Accuracy : 0.8021
#Sensitivity : 0.0000        
#Specificity : 1.0000 


# No case weights; apply loss matrix (Heavier penalty for "False Pos" than "Miss")
m_cp014_FP <- rpart(formula28x, data = pta,  
                    method = "class", 
                    parms = list(split = "gini"), 
                    control = rpart.control(cp = 0.014, 
                                            loss = matrix(data=c(0,10,1,0), byrow=TRUE, nrow=2,ncol=2))
                    )

m_cp014_FP_pred <- predict(m_cp014_FP, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_FP_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))
#          Reference
#Prediction  No Yes
#       No  604 149
#       Yes   0   0
#     Accuracy : 0.8021
#  Sensitivity : 0.0000        
#  Specificity : 1.0000


# Weights + FalsePos penalty
m_cp014_wFP <- rpart(formula28x, data = pta,  
                    method = "class", 
                    weights = pta$caseweights, 
                    parms = list(split = "gini"), 
                    control = rpart.control(cp = 0.014, 
                                            loss = matrix(data=c(0,10,1,0), byrow=TRUE, nrow=2,ncol=2))
                    )

m_cp014_wFP_pred <- predict(m_cp014_wFP, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_wFP_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))

#          Reference
#Prediction  No Yes
#       No  441  61
#       Yes 163  88
#   Accuracy : 0.7025
#Sensitivity : 0.5906        
#Specificity : 0.7301 


# No weights; Heavier penalty for "Miss"
m_cp014_M <- rpart(formula28x, data = pta,  
                    method = "class", 
                    parms = list(split = "gini"), 
                    control = rpart.control(cp = 0.014, 
                                            loss = matrix(data=c(0,1,100,0), byrow=TRUE, nrow=2,ncol=2))
                    )

m_cp014_M_pred <- predict(m_cp014_M, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_M_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))
#          Reference
#Prediction  No Yes
#       No  604 149
#       Yes   0   0
#               Accuracy : 0.8021
#            Sensitivity : 0.0000        
#            Specificity : 1.0000


# Weights + Heavier penalty for "Miss"
m_cp014_wM <- rpart(formula28x, data = pta,  
                    method = "class", 
                    weights = pta$caseweights, 
                    parms = list(split = "gini"), 
                    control = rpart.control(cp = 0.014, 
                                            loss = matrix(data=c(0,1,100,0), byrow=TRUE, nrow=2,ncol=2))
                    )

m_cp014_wM_pred <- predict(m_cp014_wM, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_wM_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))
#          Reference
#Prediction  No Yes
#       No  441  61
#       Yes 163  88
#               Accuracy : 0.7025
#            Sensitivity : 0.5906         
#            Specificity : 0.7301


# Weights + zero False Pos penalty + heavy Miss penalty
m_cp014_wFP0M <- rpart(formula28x, data = pta,  
                    method = "class", 
                    weights = pta$caseweights, 
                    parms = list(split = "gini"), 
                    control = rpart.control(cp = 0.014, 
                                            loss = matrix(data=c(0,0,100,0), byrow=TRUE, nrow=2,ncol=2))
                    )

m_cp014_wFP0M_pred <- predict(m_cp014_wFP0M, newdata = pta, type = "class")

confusionMatrix(data = as.factor(m_cp014_wFP0M_pred), 
                reference = as.factor(pta$HA.Purchase), 
                positive = c("Yes"))
#          Reference
#Prediction  No Yes
#       No  441  61
#       Yes 163  88
#               Accuracy : 0.7025    
#               Sensitivity : 0.5906         
#               Specificity : 0.7301
               

```

# Pruning a classification tree  

[back to top](#top)  
  
A "full" classification tree is one that is allowed to grow without any restriction. In other words, the tree continues to sort observations at each node without any minimum criterion for improvement. In our case, sorting only stops when there are less than 20 cases in a node(*minsplit* = 20 is the default in *rpart*, but a different value can be specified). However, a full tree is hard to interpret, and likely would not generalize to other data.  

Some parameters can be specified to restrict the growth of a tree: specifying a minimum number of cases in a node before further sorting is allowed, or specifying a maximum depth (e.g., no more than three splits as the longest path through the tree; note that *minsplit* and *maxdepth* can interact).  

Alternatively, a method of "pruning" can be used to test how well the tree classifies cases at different complexities, and then settle on the simplest tree that is reasonably accurate. A low complexity parameter (CP) value allows a tree to split whenever there's a small improvement, while a high CP value requires a large improvement before allowing a further split. So low CP's lead to more complex trees, while high CP's lead to simpler trees. The best tree has the highest CP with the best accuracy, i.e., simple but accurate. The plot below shows the accuracy of different trees for our data, built using a typical range of CP values. (There is an "average" and "SE" because there's a 5-fold cross-validation procedure.)  

```{r, echo=FALSE, results='show'}

# Plot graph showing CP values & accuracy with cross-validation

# note metric=ROC needs class probabilities
set.seed(1331)
mc <- train(formula28x, data = pta, 
            weights = pta$caseweights,
            method = "rpart",
            tuneGrid = data.frame(cp = c(0, 0.001, 0.005, 0.01, 0.011, 0.012, 0.013, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05, 0.1)),
            metric = "Accuracy", 
            control = rpart.control(split="gini", minsplit = 20),
            trControl = trainControl(method="repeatedcv", number = 5, repeats = 10))
# mc$finalModel = A fit object using the best parameters
# mc$bestTune: cp = 0.02

# Plot model accuracy vs CP values
means <- mc$results$Accuracy
bar.up <- means + mc$results$AccuracySD/(5^0.5)
bar.down <- means - mc$results$AccuracySD/(5^0.5)
plot(x = mc$results$cp,
     xlab = "simple --- CP --- complex",
     y = means, 
     ylab = "Mean accuracy (with SE)", 
     type='b', pch=1, xlim=rev(c(0, 0.1)), ylim=c(0.5, 0.65) )
arrows(x0 = mc$results$cp, x1 = mc$results$cp, y0 = bar.down, y1 = bar.up, code = 3, length=0.05, angle=90, col="black")

# add point at cp = 0.02
par(new=TRUE)
plot(x = 0.02, y = 0.6102368, pch=19, col="red", 
     xlim=rev(c(0, 0.1)), ylim=c(0.5, 0.65), xaxt='n', yaxt='n', ann=FALSE)

```

Judging by accuracy, CP = 0.02 seems to be the optimal level of complexity, leading to a relatively simple tree whose accuracy is within range of more complex trees.   

```{r, echo=FALSE}

# model CP = 0.02
m_cp02 <- rpart(formula28x, data = pta, 
                 weights = pta$caseweights, 
                 method = "class", 
                 parms = list(split = "gini"), 
                 control = rpart.control(cp = 0.02, minsplit = 20, maxcompete=FALSE, maxsurrogate=0) )

rpart.plot(m_cp02, box.palette=0, type = 4, extra = 101, under=TRUE, fallen.leaves=TRUE, varlen=0, faclen=0)

```

However, examining other measures, using a CP value of 0.013 instead of 0.02 gives a 19-point increase in sensitivity (59% to 78%), for a relatively small 3-point decrease in overall accuracy (70% to 67%).  

```{r, echo=FALSE, message=FALSE, fig.width = 12, fig.height = 9}

# Calculate more metrics across a range of CP values & plot graph

cpvalues <- c(0, 0.001, 0.005, 0.01, 0.011, 0.012, 0.013, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05, 0.1)
cpdf <- data.frame(cpvalue = rep(0, length(cpvalues)), 
                   nsplits = c(0), 
                   PredNo_RealNo = c(0), 
                   PredYes_RealNo = c(0),
                   PredNo_RealYes = c(0),
                   PredYes_RealYes = c(0),
                   AUC = c(0) )

for (i in 1:length(cpvalues)) {
  
mtemp <- rpart(formula28x, data = pta, 
               weights = pta$caseweights, 
               method = "class", 
               parms = list(split = "gini"), 
               control = rpart.control(cp = cpvalues[i]))
cptab <- mtemp$cptable

predtree <- predict(mtemp, newdata = pta, type = "class")
predtreenum <- ifelse(predtree == "No", 0, 1)

cmtab <- table(predtree, pta$HA.Purchase, dnn = c("pred", "actual"))
roctemp <- roc(response = pta$Purchased_HA, predictor = predtreenum)

cpdf$cpvalue[i] <- cpvalues[i]
cpdf$nsplits[i] <- cptab[nrow(cptab), 2]
cpdf$PredNo_RealNo[i] <- cmtab[1,1]  
cpdf$PredYes_RealNo[i] <- cmtab[2,1] 
cpdf$PredNo_RealYes[i] <- cmtab[1,2]
cpdf$PredYes_RealYes[i] <- cmtab[2,2]
cpdf$AUC[i] <- as.numeric(roctemp$auc)

}

cpdf$Accuracy <- (cpdf$PredNo_RealNo + cpdf$PredYes_RealYes) / 756
cpdf$Sensitivity <- cpdf$PredYes_RealYes / (cpdf$PredNo_RealYes + cpdf$PredYes_RealYes)
cpdf$Specificity <- cpdf$PredNo_RealNo / (cpdf$PredYes_RealNo + cpdf$PredNo_RealNo)

temptab <- data.frame(cbind(cpdf[, c("cpvalue", "nsplits")],
                            round(cpdf[, c("Accuracy", "Sensitivity", "Specificity", "AUC")], 4) ) )


# Table in graph form

p1 <- ggplot(data = temptab) + 
  geom_line(aes(x = cpvalue, y = nsplits), colour='black') + 
  geom_point(aes(x = cpvalue, y = nsplits), colour='black') + 
  labs(y = "Number of splits", x = "simple --- CP --- complex") + 
  scale_x_reverse(limits = c(0.105, -0.005), breaks = seq(0, 0.1, by = 0.01)) + 
  theme_bw() + 
  theme(plot.margin = unit(c(0.25, 0.5, 0.55, 1), "cm")) + 
  theme(axis.title = element_text(size = 14, colour='black'), 
        axis.text = element_text(size = 11, colour='black')) + 
  geom_text(temptab, mapping=aes(x = cpvalues, y = nsplits, label = nsplits), 
            nudge_y = 1, nudge_x = 0.002, colour="grey50", size=4) + 
  geom_vline(xintercept = 0.013, col="black", lty="dotted")

p2 <- ggplot(data = temptab) + 
  geom_line(aes(x = cpvalue, y = Accuracy*100), colour='black') + 
  geom_point(aes(x = cpvalue, y = Accuracy*100), shape = 1, colour='black') + 
  geom_line(aes(x = cpvalue, y = Sensitivity*100), colour='firebrick3') + 
  geom_point(aes(x = cpvalue, y = Sensitivity*100), shape = 4, colour='firebrick3') + 
  geom_line(aes(x = cpvalue, y = Specificity*100), colour='deepskyblue3') + 
  geom_point(aes(x = cpvalue, y = Specificity*100), shape = 5, colour='deepskyblue3') + 
  labs(y = "Metric (%)", x = "simple --- CP --- complex") + 
  scale_y_continuous(limits = c(45, 95), breaks = seq(45, 95, 5)) + 
  scale_x_reverse(limits = c(0.105, -0.005), breaks = seq(0, 0.1, by = 0.01)) + 
  theme_bw() + 
  theme(plot.margin = unit(c(0.25, 0.5, 0.55, 1), "cm")) + 
  theme(axis.title = element_text(size = 14, colour='black'), 
        axis.text = element_text(size = 11, colour='black')) + 
  annotate('text', x=0.09, y=71, label = "Specificity", size=5, colour = 'deepskyblue3', angle='0.25') + 
  annotate('text', x=0.09, y=64, label = "Accuracy", size = 5, colour = 'black', angle='5') + 
  annotate('text', x=0.09, y=51, label = "Sensitivity", size = 5, colour = 'firebrick3', angle='5') + 
  geom_vline(xintercept = 0.013, col="black", lty="dotted")
  
library(patchwork)
p1 / p2

# Print accompanying table from above
print(temptab[order(temptab$cpvalue, decreasing = TRUE), ] )

```


# Old analysis: Max depth  

[back to top](#top)  

```{r, include=FALSE, results='hide'}

# previous dataset & analysis 
pta_orig <- read.csv("/Users/huiwen/Documents/Research/Sonova_1_Stigma_and_Connect_Hearing/analyses_2021/data_quickread/ptacomp.f.csv", header=TRUE)

# need 0/1 variable for ROC!
pta_orig$Purchased_HA <- ifelse(pta_orig$HA.Purchase == "No", 0, 1)

# note Age_Stigma_avg still included Q4
#plot(pta_orig$Age_stigma_avg, pta$Age_stigma_avg)

# original analysis, original data
m_dep4_orig <- rpart(formula28x, data = pta_orig, 
                weights = pta_orig$weight, 
                method = "class", 
                parms = list(split = "gini"), 
                control = rpart.control(split="gini", minsplit=20, maxdepth=4) )

# original analysis, revised Age_Stigma_avg
m_dep4_rev <- rpart(formula28x, data = pta, 
                weights = pta$caseweights, 
                method = "class", 
                parms = list(split = "gini"), 
                control = rpart.control(split="gini", minsplit=20, maxdepth=4) )

# original analysis, specifying maxcompete=FALSE, maxsurrogate=0; revised Age_Stigma_avg
#m_dep4 <- rpart(formula28x, data = pta, 
#                weights = pta$caseweights, 
#                method = "class", 
#                parms = list(split = "gini"), 
#                control = rpart.control(split="gini", minsplit=20, maxdepth=4, #maxcompete=FALSE, maxsurrogate=0) )
# Conclusion: adding maxcompete=FALSE, maxsurrogate=0 does not change the final tree;

# branches containing Age_Stigma_avg dropped out after item 4 was dropped, but otherwise the same

# Model metrics with Q4
#m_dep4_orig_pred <- predict(m_dep4_orig, newdata = pta_orig, type = "class") #No=1
#m_dep4_orig_pred.f <- factor( m_dep4_orig_pred, levels=c("No", "Yes") )
#pta_orig$HA.Purchase.f <- factor( pta_orig$HA.Purchase, levels=c("No", "Yes") )

#table(m_dep4_orig_pred, pta_orig$HA.Purchase, dnn=c('Predicted', 'Actual'))

#confusionMatrix(data = m_dep4_orig_pred.f, 
#                reference = pta_orig$HA.Purchase.f, 
#                positive = c("Yes") )
#Accuracy : 0.7145; Sensitivity : 0.6309; Specificity : 0.7351

#pta_orig$Purchased_HA <- ifelse(pta_orig$HA.Purchase == "No", 0, 1)
#m_dep4_orig_pred_num <- ifelse(m_dep4_orig_pred == "No", 0, 1)
#roc(response = pta_orig$Purchased_HA, predictor = m_dep4_orig_pred_num) #0.683

# Model metrics without Q4
#m_dep4_rev_pred <- predict(m_dep4_rev, newdata = pta, type = "class") #No=1
#m_dep4_rev_pred.f <- factor( m_dep4_rev_pred, levels=c("No", "Yes") )
#pta$HA.Purchase.f <- factor( pta$HA.Purchase, levels=c("No", "Yes") )

#table(m_dep4_rev_pred, pta$HA.Purchase, dnn=c('Predicted', 'Actual'))

#confusionMatrix(data = m_dep4_rev_pred.f, 
#                reference = pta$HA.Purchase.f, 
#                positive = c("Yes") )
# Accuracy : 0.6999; Sensitivity : 0.6309; Specificity : 0.7169

#pta$Purchased_HA <- ifelse(pta$HA.Purchase == "No", 0, 1)
#m_dep4_rev_pred_num <- ifelse(m_dep4_rev_pred == "No", 0, 1)
#roc(response = pta$Purchased_HA, predictor = m_dep4_rev_pred_num) #0.6739

# Plot with and w/o Q4 side by side -- can't use rpart plots??

#Q4inc.jpeg
#rpart.plot(m_dep4_orig, box.palette=0, type = 4, extra = 101, under=TRUE, fallen.leaves=TRUE, varlen=0, faclen=0)

#Q4exc.jpeg
#rpart.plot(m_dep4_rev, box.palette=0, type = 4, extra = 101, under=TRUE, fallen.leaves=TRUE, varlen=0, faclen=0)


```

**Max depth = 4.** In the original analysis where the growth of the classification tree was restricted to a maximum depth of 4, Age Stigma formed two branches at depth 3. After dropping Q4 (the first of five items in the Age Stigma scale), Age Stigma was no longer part of the decision tree, but other branches and the overall accuracy stayed similar.  

![](Q4inc.jpeg){width=450px} ![](Q4exc.jpeg){width=400px}  

```{r, echo=FALSE, message=FALSE}

# Table showing metrics with and w/o Q4

obj <- data.frame(acc = c(0.7145, 0.6999)*100, 
                  sens = c(0.6309, 0.6309)*100, 
                  spec = c(0.7351, 0.7169)*100,
                  auc = c(0.683, 0.6739)*100 )
rownames(obj) <- c("With Q4", "Without Q4")
colnames(obj) <- c("Accuracy %", "Sensitivity %", "Specificity %", "AUC")

kable(obj, format="pipe", row.names = TRUE)

```

# New analysis: Pruning 

## CP=0.02 vs CP=0.013  

[back to top](#top)  

As stated earlier, using a CP value of 0.013 instead of 0.02 gives a 19-point increase in sensitivity, for a 3-point decrease in overall accuracy. Both tree models and their metrics are shown below for comparison.  

```{r, echo=FALSE, message=FALSE}

# The two trees
m_cp02 <- rpart(formula28x, data = pta, 
                 weights = pta$caseweights, 
                 method = "class", 
                 parms = list(split = "gini"), 
                 control = rpart.control(cp = 0.02, minsplit = 20, maxcompete=FALSE, maxsurrogate=0) )

m_cp013 <- rpart(formula28x, data = pta, 
                 weights = pta$caseweights, 
                 method = "class", 
                 parms = list(split = "gini"), 
                 control = rpart.control(cp = 0.013, minsplit = 20, maxcompete=FALSE, maxsurrogate=0) )

# Table of metrics, using info from below
obj <- data.frame(acc = c(0.672, 0.6746)*100, 
                  sens = c(0.5906, 0.7852)*100, 
                  spec = c(0.6921, 0.6474)*100,
                  auc = c(0.6413, 0.7163)*100 )
rownames(obj) <- c("CP = 0.02", "CP = 0.013")
colnames(obj) <- c("Accuracy %", "Sensitivity %", "Specificity %", "AUC")

kable(obj, format="pipe", row.names = TRUE)

```

```{r, evaluate=FALSE, include=FALSE}

# Calculations of model metrics, tree plots

# Basis of manual tree plots
rpart.plot(m_cp013, box.palette=0, type = 4, extra = 101, under=TRUE, fallen.leaves=TRUE, 
           varlen=0, faclen=0)
rpart.plot(m_cp02, box.palette=0, type = 4, extra = 101, under=TRUE, fallen.leaves=TRUE, 
           varlen=0, faclen=0)

# Model metrics for CP = 0.02
m_cp02_pred <- predict(m_cp02, newdata = pta, type = "class")
m_cp02_pred.f <- factor( m_cp02_pred, levels=c("No", "Yes") )
pta$HA.Purchase.f <- factor( pta$HA.Purchase, levels=c("No", "Yes") )
table(m_cp02_pred, pta$HA.Purchase.f, dnn=c('Predicted', 'Actual'))
confusionMatrix(data = m_cp02_pred.f, reference = pta$HA.Purchase.f, positive = c("Yes") )
# Accuracy : 0.672; Sensitivity : 0.5906; Specificity : 0.6921
m_cp02_pred_bi <- ifelse(m_cp02_pred == "No", 0, 1) # roc() requires 1/0
roc(response = pta$Purchased_HA, predictor = m_cp02_pred_bi) #0.6413

# Model metrics for CP = 0.013
m_cp013_pred <- predict(m_cp013, newdata = pta, type = "class")
m_cp013_pred.f <- factor( m_cp013_pred, levels=c("No", "Yes") )
pta$HA.Purchase.f <- factor( pta$HA.Purchase, levels=c("No", "Yes") )
table(m_cp013_pred, pta$HA.Purchase.f, dnn=c('Predicted', 'Actual'))
confusionMatrix(data = m_cp013_pred.f, reference = pta$HA.Purchase.f, positive = c("Yes") )
# Accuracy : 0.6746; Sensitivity : 0.7852; Specificity : 0.6474
m_cp013_pred_bi <- ifelse(m_cp013_pred == "No", 0, 1) # roc() requires 1/0
roc(response = pta$Purchased_HA, predictor = m_cp013_pred_bi) #0.7163

```

![](rpart_plot_cp02.jpg){width=300px} ![](rpart_plot_cp013.jpg){width=500px}  
[Larger: CP=0.02](https://github.com/huiwen-goy/connect1-stigma/blob/master/docs/tree_m_cp02.pdf) with actual and weighted counts | [Larger: CP=0.013](https://github.com/huiwen-goy/connect1-stigma/blob/master/docs/tree_m_cp013.pdf) with actual and weighted counts  


## Variable importance  

[back to top](#top)  

In this case, variable importance was calculated from how much each variable increased the proportion of correctly classified cases, relative to the other variables. In the plots below, variable importance was scaled so that the total summed to 100%. (As an illustration, of all the cases that were switched from an incorrect category to the correct category by the CP=0.02 tree model, Age accounted for 37% for them.)  

*Technical note: maxcompete = FALSE and maxsurrogate = 0, to look only at the variables involved in the primary splits.*  

```{r, echo=FALSE, results='hide', fig.show='asis', fig.height=8}

# Plot variable importance
# sum(vi$Variable_importance) = 'x'; using 'x' as denominator, % printed in summary(model)

# CP = 0.02
vi_cp02 <- data.frame(m_cp02$variable.importance)
colnames(vi_cp02) <- c("Variable_importance")
vi_cp02$number <- seq(1, 3, by = 1)

sum(vi_cp02$Variable_importance) #12.47614

g1 <- ggplot(data = vi_cp02, aes(y = Variable_importance/12.47614*100, x = number)) + 
  geom_col() + 
  coord_flip(xlim=rev(c(0, 4)), ylim=c(0, 40)) + 
  scale_x_continuous(name='', breaks = seq(1, 3, by = 1), labels=rownames(vi_cp02)) + 
  scale_y_continuous(name="Variable importance (%)", breaks = seq(0, 40, by=5)) +
  ggtitle("CP = 0.02") + 
  theme_bw() + 
  theme(plot.margin = unit(c(0.25, 0.5, 0.55, 1), "cm")) + 
  theme(axis.title = element_text(size = 16, colour='black'), 
        axis.text = element_text(size = 12, colour='black')) + 
  theme(plot.title = element_text(hjust = 0.5, size = 20)) + 
  theme(plot.margin = unit(c(1, 0.25, 0.25, 0.25), "cm"))


# CP = 0.013
vi_cp013 <- data.frame(m_cp013$variable.importance)
colnames(vi_cp013) <- c("Variable_importance")
vi_cp013$number <- c(1:10)

sum(vi_cp013$Variable_importance) #33.57578

g2 <- ggplot(data = vi_cp013, aes(y = Variable_importance/33.57578*100, x = number)) + 
  geom_col() + 
  coord_flip(xlim=rev(c(0, 11)), ylim=c(0, 40)) + 
  scale_x_continuous(name='', breaks = seq(1, 10, by = 1), labels=rownames(vi_cp013)) + 
  scale_y_continuous(name="Variable importance (%)", breaks = seq(0, 40, by = 5)) + 
  ggtitle("CP = 0.013") + 
  theme_bw() + 
  theme(plot.margin = unit(c(0.25, 0.5, 0.55, 1), "cm")) + 
  theme(axis.title = element_text(size = 16, colour='black'), 
        axis.text = element_text(size = 12, colour='black')) + 
  theme(plot.title = element_text(hjust = 0.5, size = 20)) + 
  theme(plot.margin = unit(c(1, 0.25, 0.25, 0.25), "cm"))

# make panel same width; can't figure out how to customize heights
#library(gtable)
#library(grid)
#grob1 <- ggplotGrob(g1)
#grob2 <- ggplotGrob(g2)
#g <- rbind(grob1, grob2, size = "first")
#g$widths <- unit.pmax(grob1$widths, grob2$widths)
#grid.newpage()
#grid.draw(g)

library(patchwork)
g1 / g2 

```

## Model stability  

[back to top](#top)  

To check how both tree models changed (or not) with different subsets of the data, 5 different trees were constructed using the same parameters, dropping out a different, randomly-selected 20% portion of the data each time. The proportion of 'Yes' and 'No' cases was kept constant in all subsets of the data. The same cases were dropped out for both trees.  

```{r, include=FALSE, message=FALSE}

# create 5 folds, stratified by outcome
set.seed(800)
holdout <- createFolds(y = pta$HA.Purchase, k = 5, list = FALSE)

# checked that fold cases were unique; sum(fold5$ID %in% pta[holdout!=5, ]$ID.clinic)
# checked that sampling seems random; which(pta$ID %in% fold1$ID)

# create dataframe to store numbers obtained from each subset

var_list <- c("Age", "PTA4_better_ear", "HHIE_total", "Ability", "Sex.f", "Edu", "Married.f", "Health", "QoL", "Help_neighbours", "Help_problems", "Concern", "Lonely", "Sub_Age_avg", "Age_stigma_avg", "HA_stigma_avg", "Accomp.f", "Soc_Suspect_HL.f", "Soc_Know_HL.f", "Soc_Discuss_HL.f", "Soc_Hearing_test.f", "Soc_Obtain_HA.f", "Soc_Sometimes_use.f", "Soc_Regular_use.f", "Soc_Very_positive.f", "Soc_Somewhat_positive.f", "Soc_Somewhat_negative.f", "Soc_Very_negative.f")

dfvi_cp02 <- data.frame(variable = var_list, temp = c(0) )

# drop out each fold, and get tree and variable importance for remaining data
for (i in 1:5) {
  tempdata <- pta[holdout != i, ]
  
  # construct tree model
  mtree <- rpart(formula28x, data = tempdata, weights = tempdata$caseweights, 
                 method = "class", parms = list(split = "gini"), 
                 control = rpart.control(cp = 0.02, maxcompete = FALSE, maxsurrogate=0))

  # get variable importance as dataframe
  varimp_temp <- data.frame(mtree$variable.importance)
  varimp_temp$variable <- rownames(varimp_temp) 
  colnames(varimp_temp)[1] <- paste0("subset_", i)
  
  # left merge onto master list of variables
  dfvi_cp02 <- merge(x = dfvi_cp02, 
                     y = varimp_temp, 
                     all.x = TRUE, 
                     by.y = c("variable") )
}

# drop temp column of zero's
dfvi_cp02 <- dfvi_cp02[, -2]

# divide each column by sum of column's importances and convert to %
percentages_cp02 <- mapply('/', dfvi_cp02[, 2:6], colSums(dfvi_cp02[, 2:6], na.rm=TRUE)) * 100

```

**Changes in variable importance (%) across data subsets, CP = 0.02**
```{r, echo=FALSE}

obj <- data.frame(cbind(dfvi_cp02$variable, round(percentages_cp02, 4))) 
kable(obj, format="pipe", row.names = FALSE)

```

```{r, include=FALSE, message=FALSE}

dfvi_cp013 <- data.frame(variable = var_list, temp = c(0) )

# drop out each fold, and get tree and variable importance for remaining data
for (i in 1:5) {
  tempdata <- pta[holdout != i, ]
  
  # construct tree model
  mtree <- rpart(formula28x, data = tempdata, weights = tempdata$caseweights, 
                 method = "class", parms = list(split = "gini"), 
                 control = rpart.control(cp = 0.013, maxcompete = FALSE, maxsurrogate=0))

  # get variable importance as dataframe
  varimp_temp <- data.frame(mtree$variable.importance)
  varimp_temp$variable <- rownames(varimp_temp) 
  colnames(varimp_temp)[1] <- paste0("subset_", i)
  
  # left merge onto master list of variables
  dfvi_cp013 <- merge(x = dfvi_cp013, 
                     y = varimp_temp, 
                     all.x = TRUE, 
                     by.y = c("variable") )
}

# drop temp column of zero's
dfvi_cp013 <- dfvi_cp013[, -2]

# divide each column by sum of column's importances and convert to %
percentages_cp013 <- mapply('/', dfvi_cp013[, 2:6], colSums(dfvi_cp013[, 2:6], na.rm=TRUE)) * 100

```

**Changes in variable importance (%) across data subsets, CP = 0.013**
```{r, echo=FALSE}

obj <- data.frame(cbind(dfvi_cp013$variable, round(percentages_cp013, 4))) 
kable(obj, format="pipe", row.names = FALSE)
```

Age, HHIE and PTA(BE) were consistently important for classifying whether participants decided to purchase hearing aids. Even so, the extent of importance varied depending on which data were used, indicating that the models weren't very stable. In other words, a small change in the data led to quite a different model.  

# Takeaways  

[back to top](#top)  

1. A standard method of tree pruning based only on accuracy suggested that a 3-split tree was an adequate compromise between complexity and accuracy. However, looking at sensitivity, a 13-split tree was much better at identifying cases of hearing aid purchase, without much increase in false positives.  

2. A measure of variable importance showed that Age, HHIE and PTA(BE) were the most important variables for correctly classifying hearing aid purchase in both simpler and more complex tree models.  

3. Checking on the stability of the tree models by dropping out different subsets of data, the effects of Age, HHIE and PTA(BE) varied quite a bit, indicating that these tree models weren't very stable.  
